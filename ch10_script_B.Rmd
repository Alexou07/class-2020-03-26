---
title: 'Chapter 10: Confidence Intervals'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(infer)
library(gov.1005.data)
library(broom)
library(tidyverse)

# A little trick to change the order of the levels of treatment, and make the
# later prompts easier.

train <- train %>% 
  mutate(treatment = fct_relevel(treatment, levels = c("Control")))
```


# Scene 8

**Prompt:**  What about the difference between `att_chg` of the treated and the controls? Calculate the mean difference, just as we did with income. Is this difference "large?" Do the numbers seems sensible? Provide an hypothesis as to why the average `att_chg` is different in both groups.



# Scene 9

**Prompt:** What is the 99% confidence interval for that difference? Provide a Bayesian and Frequentist definition of that interval. Write them down in your Rmd! Are these results consistent with what Enos finds? Check out the paper and see! Why am I using a 99% confidence interval here? Why not 99.5% or 97%?


# Scene 10

**Prompt:** Use the language of the Rubin Causal Model and potential outcomes to describe this experiment. What are the units? What are the treatments? What are the outcomes? Remind us what the fundamental problem of causal inference is. Write all this down in a paragraph you construct together. As always, each student needs their own paragraph. Preceptor still has `.my_cold_call()` . . .

# Scene 11

**Prompt:** Use `lm()` to calculate the difference between att_chg of the treated and the controls. (You did read [section 10.4](https://davidkane9.github.io/PPBDS/10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut) of the *Primer*, right? Note, we are not using the bootstrap here. We are just exploring a different way of doing the same calculation as we did for Prompt 7. Always a good idea to check out the *Primer*! 


# Scene 12

**Prompt:** Calculate the 99% confidence interval for the difference between att_chg of the treated and the controls using a bootstrap approach and `lm()`? (Hint: After `group_by(replicate)`, you will want to use `nest()` to group all the observations from that group and then hand them to `lm()` using map functions and list columns.) Not easy! [Look at](https://davidkane9.github.io/PPBDS/11-regression.html#uncertainty-in-simple-linear-regressions) the *Primer* for an example of how to use `nest()` in this way.

# Scene 13

**Prompt:** Calculate the 99% confidence interval using simple `lm()`. In other words, we use the bootstrap to understand why things work. With that intuition, we can take short cuts, like with `lm()` and the various arguments to `tidy()`.


